"""
ì½˜í…ì¸ íŒ€(ë¸”ë¡œê·¸) ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ
- Work CSV: '[ì½˜í…ì¸ íŒ€] í¬ìŠ¤íŒ… ì—…ë¬´ í˜„í™©*.csv'
- Inflow xlsx: 'ìœ ì…ë¶„ì„_ì›”ê°„_*.xlsx'
- Views rank xlsx: 'ì¡°íšŒìˆ˜_ìˆœìœ„_ì›”ê°„_*.xlsx'
- Views monthly xlsx: 'ì¡°íšŒìˆ˜_ì›”ê°„_*.xlsx'

ì¶”ê°€ ë¶„ì„ ê¸°ëŠ¥:
1. ê²€ìƒ‰ì–´ ì‹¬ì¸µ ë¶„ì„: URLì—ì„œ query/q íŒŒë¼ë¯¸í„° ì¶”ì¶œ
2. íš¨ì ì½˜í…ì¸ (ìŠ¤í…Œë””ì…€ëŸ¬) ë°œêµ´: ê³¼ê±° ê²Œì‹œë¬¼ ì¤‘ ì¸ê¸°ê¸€ í‘œì‹œ
3. ì„±ê³¼ ì›ì¸ ìë™ ì§„ë‹¨: ì¡°íšŒìˆ˜ í•˜ë½ ì›ì¸ ë¶„ì„
"""

import re
import pandas as pd
import numpy as np
from io import BytesIO
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from urllib.parse import urlparse, parse_qs, unquote
import warnings
warnings.filterwarnings('ignore')


@dataclass
class LoadedFile:
    name: str
    df: Optional[pd.DataFrame] = None
    raw_bytes: Optional[bytes] = None


def extract_search_keyword_from_url(url: str) -> Optional[str]:
    """
    URLì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    query=, q=, search=, keyword= ë“±ì˜ íŒŒë¼ë¯¸í„°ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    if not url or pd.isna(url):
        return None

    try:
        url_str = str(url).strip()
        if not url_str or url_str.lower() == 'nan':
            return None

        # URL íŒŒì‹±
        parsed = urlparse(url_str)
        query_params = parse_qs(parsed.query)

        # ê²€ìƒ‰ì–´ íŒŒë¼ë¯¸í„° ìš°ì„ ìˆœìœ„
        search_params = ['query', 'q', 'search', 'keyword', 'searchKeyword', 'where']

        for param in search_params:
            if param in query_params:
                keyword = query_params[param][0]
                # URL ë””ì½”ë”©
                keyword = unquote(keyword)
                # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ë°˜í™˜
                if keyword and keyword.strip():
                    return keyword.strip()

        # ë„¤ì´ë²„ ê²€ìƒ‰ URL íŠ¹ìˆ˜ ì²˜ë¦¬
        if 'naver.com' in url_str:
            # ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ URL íŒ¨í„´
            match = re.search(r'[?&]query=([^&]+)', url_str)
            if match:
                return unquote(match.group(1))

        # ë‹¤ìŒ ê²€ìƒ‰ URL íŠ¹ìˆ˜ ì²˜ë¦¬
        if 'daum.net' in url_str:
            match = re.search(r'[?&]q=([^&]+)', url_str)
            if match:
                return unquote(match.group(1))

        # êµ¬ê¸€ ê²€ìƒ‰ URL íŠ¹ìˆ˜ ì²˜ë¦¬
        if 'google.com' in url_str or 'google.co.kr' in url_str:
            match = re.search(r'[?&]q=([^&]+)', url_str)
            if match:
                return unquote(match.group(1))

    except Exception:
        pass

    return None


def parse_write_date(date_str: str) -> Optional[pd.Timestamp]:
    """ì‘ì„±ì¼ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ Timestampë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if not date_str or pd.isna(date_str):
        return None

    try:
        date_str = str(date_str).strip()
        if not date_str or date_str.lower() == 'nan':
            return None

        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±
        dt = pd.to_datetime(date_str, errors='coerce')
        if pd.notna(dt):
            return dt
    except Exception:
        pass

    return None


def is_steady_seller(write_date_str: str, analysis_month: str) -> bool:
    """
    ìŠ¤í…Œë””ì…€ëŸ¬(íš¨ì ì½˜í…ì¸ )ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
    ì‘ì„±ì¼ì´ ë¶„ì„ ëŒ€ìƒ ì›” ì´ì „ì´ë©´ ìŠ¤í…Œë””ì…€ëŸ¬ì…ë‹ˆë‹¤.
    """
    if not write_date_str or not analysis_month:
        return False

    try:
        write_dt = parse_write_date(write_date_str)
        if write_dt is None:
            return False

        # ë¶„ì„ ì›”ì˜ ì²«ë‚ 
        analysis_dt = pd.to_datetime(f"{analysis_month}-01")

        # ì‘ì„±ì¼ì´ ë¶„ì„ ì›” ì´ì „ì¸ì§€ í™•ì¸
        return write_dt < analysis_dt
    except Exception:
        return False


def generate_performance_diagnosis(
    curr_views: int,
    prev_views: int,
    curr_publish_count: int,
    prev_publish_count: int = None
) -> Dict[str, Any]:
    """
    ì„±ê³¼ ì›ì¸ì„ ìë™ ì§„ë‹¨í•©ë‹ˆë‹¤.
    ì¡°íšŒìˆ˜ í•˜ë½ ì‹œ ë°œí–‰ëŸ‰ ë¶€ì¡±ì¸ì§€ ê²€ìƒ‰ ë…¸ì¶œ í•˜ë½ì¸ì§€ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    diagnosis = {
        'has_issue': False,
        'issue_type': None,
        'severity': 'normal',
        'message': '',
        'recommendation': ''
    }

    if prev_views <= 0:
        return diagnosis

    # ì¦ê°ë¥  ê³„ì‚°
    growth_rate = ((curr_views - prev_views) / prev_views) * 100

    # 10% ì´ìƒ í•˜ë½ ì‹œ ì§„ë‹¨
    if growth_rate <= -10:
        diagnosis['has_issue'] = True
        diagnosis['severity'] = 'warning' if growth_rate > -30 else 'critical'

        # ë°œí–‰ëŸ‰ ë¶„ì„
        if curr_publish_count <= 2:
            # ë°œí–‰ëŸ‰ ë¶€ì¡±ì´ ì›ì¸
            diagnosis['issue_type'] = 'low_publish_count'
            diagnosis['message'] = f"âš ï¸ ì¡°íšŒìˆ˜ê°€ {abs(growth_rate):.1f}% í•˜ë½í–ˆìŠµë‹ˆë‹¤. ì´ë²ˆ ë‹¬ ë°œí–‰ëŸ‰({curr_publish_count}ê±´)ì´ ë§¤ìš° ì ìŠµë‹ˆë‹¤."
            diagnosis['recommendation'] = "ğŸ“Œ ë°œí–‰ ì£¼ê¸°ë¥¼ ë‹¨ì¶•í•˜ì—¬ ì½˜í…ì¸  ìƒì‚°ëŸ‰ì„ ëŠ˜ë¦´ ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì›” ìµœì†Œ 4ê±´ ì´ìƒì˜ í¬ìŠ¤íŒ…ì„ ëª©í‘œë¡œ ì„¤ì •í•˜ì„¸ìš”."
        else:
            # ê²€ìƒ‰ ë…¸ì¶œ í•˜ë½ì´ ì›ì¸
            diagnosis['issue_type'] = 'search_exposure_drop'
            diagnosis['message'] = f"âš ï¸ ì¡°íšŒìˆ˜ê°€ {abs(growth_rate):.1f}% í•˜ë½í–ˆìŠµë‹ˆë‹¤. ë°œí–‰ëŸ‰({curr_publish_count}ê±´)ì€ ì ì ˆí•˜ë‚˜ ê²€ìƒ‰ ë…¸ì¶œì´ ê°ì†Œí•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."
            diagnosis['recommendation'] = "ğŸ“Œ í‚¤ì›Œë“œ ìµœì í™”ì™€ ì½˜í…ì¸  í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ì¸ê¸° ê²€ìƒ‰ì–´ë¥¼ í™œìš©í•œ ì œëª© ìˆ˜ì •ì„ ê³ ë ¤í•˜ì„¸ìš”."

    elif growth_rate >= 20:
        # ì„±ì¥ ì¤‘
        diagnosis['severity'] = 'success'
        diagnosis['message'] = f"âœ… ì¡°íšŒìˆ˜ê°€ {growth_rate:.1f}% ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. ì¢‹ì€ ì„±ê³¼ì…ë‹ˆë‹¤!"
        if curr_publish_count > 0:
            diagnosis['recommendation'] = "í˜„ì¬ ì½˜í…ì¸  ì „ëµì„ ìœ ì§€í•˜ë©´ì„œ ì¸ê¸° ì½˜í…ì¸ ì˜ íŒ¨í„´ì„ ë¶„ì„í•´ ë³´ì„¸ìš”."

    return diagnosis


def parse_date_to_year_month(date_value) -> Optional[str]:
    """Parse various date formats to YYYY-MM."""
    if pd.isna(date_value):
        return None

    try:
        if isinstance(date_value, pd.Timestamp):
            return date_value.strftime('%Y-%m')

        date_str = str(date_value).strip()
        if not date_str or date_str.lower() == 'nan':
            return None

        dt = pd.to_datetime(date_str, errors='coerce')
        if pd.notna(dt):
            return dt.strftime('%Y-%m')
    except Exception:
        pass

    return None


def parse_date_range_to_year_month(range_str: str) -> Optional[str]:
    """Parse date range like '2025-12-01~2025-12-31' to YYYY-MM from start date."""
    if pd.isna(range_str):
        return None

    try:
        match = re.search(r'(\d{4}-\d{2}-\d{2})', str(range_str))
        if match:
            date_str = match.group(1)
            dt = pd.to_datetime(date_str)
            return dt.strftime('%Y-%m')
    except Exception:
        pass

    return None


def find_header_row_by_columns(df: pd.DataFrame, required_cols: List[str]) -> int:
    """Find row index containing required column names."""
    for idx in range(min(20, len(df))):
        row_values = [str(v).strip() for v in df.iloc[idx].values if pd.notna(v)]
        matches = sum(1 for col in required_cols if any(col in v for v in row_values))
        if matches >= len(required_cols):
            return idx
    return -1


def process_work_csv(files: List[LoadedFile]) -> Dict[str, Any]:
    """Process work CSV: '[ì½˜í…ì¸ íŒ€] í¬ìŠ¤íŒ… ì—…ë¬´ í˜„í™©*.csv'"""
    all_work = []

    # Forward fill columns for Notion-style data
    ffill_columns = ['*ID', 'ìƒíƒœ', 'ê±°ë˜ì²˜ ëª…', 'ê³„ì•½ìƒí’ˆ', 'ê³„ì•½ ê±´ìˆ˜', 'ë°œí–‰ ì™„ë£Œ ê±´ìˆ˜', 'ë°œí–‰ ì™„ë£Œ', 'ë‚¨ì€ ì‘ì—… ê±´ìˆ˜', 'ì§€ë‚œë‹¬ ì´ì›” ê±´ìˆ˜']

    for f in files:
        if not f.name.lower().endswith('.csv'):
            continue
        if 'ì½˜í…ì¸ ' not in f.name and 'í¬ìŠ¤íŒ…' not in f.name:
            continue

        try:
            if f.df is not None:
                df = f.df.copy()
            elif f.raw_bytes:
                df = pd.read_csv(BytesIO(f.raw_bytes), encoding='utf-8-sig')
            else:
                continue

            # Forward fill for Notion-style grouped data
            # IMPORTANT: ffill within *ID groups only to prevent cross-client data leakage
            id_col = None
            for c in df.columns:
                if str(c).strip() == '*ID':
                    id_col = c
                    break
            
            if id_col:
                # First, forward fill the ID column itself
                df[id_col] = df[id_col].ffill()
                
                # Then forward fill other columns WITHIN each ID group
                for col in ffill_columns:
                    if col == '*ID':
                        continue  # Already handled above
                    matching_cols = [c for c in df.columns if col in str(c)]
                    for mc in matching_cols:
                        df[mc] = df.groupby(id_col)[mc].ffill()
            else:
                # Fallback: simple ffill if no ID column found
                for col in ffill_columns:
                    matching_cols = [c for c in df.columns if col in str(c)]
                    for mc in matching_cols:
                        df[mc] = df[mc].ffill()

            # Find column mappings based on actual data
            col_mapping = {}
            for col in df.columns:
                col_str = str(col).strip()

                if col_str == 'í¬ìŠ¤íŒ…-ì—…ë¡œë“œ':
                    col_mapping['upload_date'] = col
                elif col_str == 'í¬ìŠ¤íŒ…-ì‘ì—…ì™„ë£Œì¼':
                    col_mapping['complete_date'] = col
                elif col_str == 'í¬ìŠ¤íŒ…-ìë£Œ ìˆ˜ì‹ ì¼':
                    col_mapping['receive_date'] = col
                elif col_str == 'ê±°ë˜ì²˜ ëª…':
                    col_mapping['clinic'] = col
                elif col_str == 'ê³„ì•½ ê±´ìˆ˜' and 'contract_count' not in col_mapping:
                    col_mapping['contract_count'] = col
                elif (col_str == 'ë°œí–‰ ì™„ë£Œ ê±´ìˆ˜' or col_str == 'ë°œí–‰ ì™„ë£Œ') and 'published_count' not in col_mapping:
                    col_mapping['published_count'] = col
                elif col_str == 'ë‚¨ì€ ì‘ì—… ê±´ìˆ˜' and 'remaining_count' not in col_mapping:
                    col_mapping['remaining_count'] = col
                elif col_str == 'ì§€ë‚œë‹¬ ì´ì›” ê±´ìˆ˜' and 'base_carryover' not in col_mapping:
                    col_mapping['base_carryover'] = col
                elif col_str == 'ì‹œì‘ì¼' and 'start_date' not in col_mapping:
                    col_mapping['start_date'] = col
                elif (col_str == 'ìƒíƒœ' or col_str == 'í¬ìŠ¤íŒ…-ìƒíƒœ') and 'status' not in col_mapping:
                    col_mapping['status'] = col
                elif col_str == 'ê³„ì•½ìƒí’ˆ':
                    col_mapping['contract_item'] = col
                elif col_str == 'í¬ìŠ¤íŒ…-í¬ìŠ¤íŒ… URL':
                    col_mapping['post_url'] = col
                elif col_str == 'í¬ìŠ¤íŒ…-ê²Œì‹œë¬¼ ì œëª©':
                    col_mapping['post_title'] = col

            for _, row in df.iterrows():
                # year_month ê²°ì • ë¡œì§:
                # 1. í¬ìŠ¤íŒ… í–‰: upload_date (ë°œí–‰ì¼) ê¸°ì¤€
                # 2. ID ê·¸ë£¹ ëŒ€í‘œ í–‰: start_date ê¸°ì¤€
                year_month = None
                upload_date_raw = ''
                group_year_month = None  # ID ê·¸ë£¹ì˜ ëŒ€í‘œ ì›” (start_date ê¸°ì¤€)

                # upload_date (ë°œí–‰ì¼)ì„ ìš°ì„  ì‚¬ìš© - ê° í¬ìŠ¤íŒ…ì˜ ì‹¤ì œ ë°œí–‰ ì›”
                if 'upload_date' in col_mapping:
                    upload_val = row.get(col_mapping['upload_date'], '')
                    if pd.notna(upload_val) and str(upload_val).strip() and str(upload_val).strip().lower() != 'nan':
                        year_month = parse_date_to_year_month(upload_val)
                        upload_date_raw = str(upload_val).strip()

                # upload_dateê°€ ì—†ìœ¼ë©´ start_date ì‚¬ìš© (ê³„ì•½ ì‹œì‘ ì›”)
                if not year_month and 'start_date' in col_mapping:
                    start_val = row.get(col_mapping['start_date'], '')
                    if pd.notna(start_val) and str(start_val).strip() and str(start_val).strip().lower() != 'nan':
                        year_month = parse_date_to_year_month(start_val)

                # ID ê·¸ë£¹ì˜ ëŒ€í‘œ ì›” (start_date ê¸°ì¤€) - ê³„ì•½ ì •ë³´ ì§‘ê³„ìš©
                if 'start_date' in col_mapping:
                    start_val = row.get(col_mapping['start_date'], '')
                    if pd.notna(start_val) and str(start_val).strip() and str(start_val).strip().lower() != 'nan':
                        group_year_month = parse_date_to_year_month(start_val)

                clinic = str(row.get(col_mapping.get('clinic', ''), '')).strip()
                contract_item = str(row.get(col_mapping.get('contract_item', ''), '')).strip()
                status = str(row.get(col_mapping.get('status', ''), '')).strip()
                post_url = str(row.get(col_mapping.get('post_url', ''), '')).strip()
                post_title = str(row.get(col_mapping.get('post_title', ''), '')).strip()

                contract_count = pd.to_numeric(row.get(col_mapping.get('contract_count', ''), 0), errors='coerce') or 0
                published_count = pd.to_numeric(row.get(col_mapping.get('published_count', ''), 0), errors='coerce') or 0
                remaining_count = pd.to_numeric(row.get(col_mapping.get('remaining_count', ''), 0), errors='coerce') or 0
                base_carryover = pd.to_numeric(row.get(col_mapping.get('base_carryover', ''), 0), errors='coerce') or 0

                # Skip empty rows
                if not clinic or clinic.lower() == 'nan':
                    continue

                all_work.append({
                    'year_month': year_month,  # ë°œí–‰ì¼ ê¸°ì¤€ ì›” (í¬ìŠ¤íŒ… ë¶„ë¥˜ìš©)
                    'group_year_month': group_year_month,  # ì‹œì‘ì¼ ê¸°ì¤€ ì›” (ê³„ì•½ ì •ë³´ ì§‘ê³„ìš©)
                    'clinic': clinic,
                    'contract_item': contract_item if contract_item.lower() != 'nan' else '',
                    'status': status if status.lower() != 'nan' else '',
                    'post_url': post_url if post_url.lower() != 'nan' else '',
                    'post_title': post_title if post_title.lower() != 'nan' else '',
                    'upload_date': upload_date_raw if upload_date_raw.lower() != 'nan' else '',
                    'contract_count': int(contract_count),
                    'published_count': int(published_count),
                    'remaining_count': int(remaining_count),
                    'base_carryover': int(base_carryover)
                })

        except Exception as e:
            print(f"Error processing work file {f.name}: {e}")
            continue

    if not all_work:
        return {}

    work_df = pd.DataFrame(all_work)

    # Get unique clinic summaries (first row per clinic)
    clinic_summary = work_df.drop_duplicates(subset=['clinic'], keep='first')

    # Aggregate by group_year_month (ì‹œì‘ì¼ ê¸°ì¤€)
    # ì¤‘ìš”: ID ê·¸ë£¹ë³„ë¡œ ê³„ì•½ ì •ë³´ëŠ” ì²« í–‰ì—ë§Œ ìˆìœ¼ë¯€ë¡œ,
    # group_year_month (ì‹œì‘ì¼) ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•´ì•¼ í•¨
    # year_month (ë°œí–‰ì¼)ëŠ” í¬ìŠ¤íŒ… ëª©ë¡ ë¶„ë¥˜ì—ë§Œ ì‚¬ìš©

    # group_year_monthê°€ ìˆëŠ” í–‰ë§Œ ì‚¬ìš© (ê³„ì•½ ì •ë³´ê°€ ìˆëŠ” ID ê·¸ë£¹ ëŒ€í‘œ í–‰)
    contract_info_rows = work_df[work_df['group_year_month'].notna()].copy()

    if not contract_info_rows.empty:
        # group_year_month ê¸°ì¤€ ì›”ë³„ ì§‘ê³„ (ê³„ì•½ ì •ë³´)
        monthly_summary = contract_info_rows.groupby('group_year_month').agg({
            'contract_count': 'first',  # ID ê·¸ë£¹ì˜ ì²« í–‰ ê°’ ì‚¬ìš©
            'published_count': 'first',
            'remaining_count': 'first',
            'base_carryover': 'first'
        }).reset_index()
        monthly_summary = monthly_summary.rename(columns={'group_year_month': 'year_month'})

        monthly_summary['completion_rate'] = np.where(
            monthly_summary['contract_count'] > 0,
            monthly_summary['published_count'] / monthly_summary['contract_count'] * 100,
            0
        )

        # ìƒíƒœë³„ ê±´ìˆ˜ ê³„ì‚° (ì›”ë³„) - year_month (ë°œí–‰ì¼) ê¸°ì¤€ìœ¼ë¡œ í¬ìŠ¤íŒ… ìˆ˜ ê³„ì‚°
        valid_work = work_df[work_df['year_month'].notna()]
        for ym in monthly_summary['year_month'].unique():
            # í•´ë‹¹ ì›”ì— ë°œí–‰ëœ í¬ìŠ¤íŒ… ìˆ˜ (year_month ê¸°ì¤€)
            month_rows = valid_work[valid_work['year_month'] == ym]

            # ìƒíƒœë³„ í•„í„°ë§ (ëŒ€ì†Œë¬¸ì ë° ê³µë°± ë¬´ì‹œ)
            completed_count = len(month_rows[
                month_rows['status'].str.strip().str.lower().isin(['ì™„ë£Œ', 'ë°œí–‰ì™„ë£Œ', 'ë°œí–‰ ì™„ë£Œ'])
            ])
            pending_data_count = len(month_rows[
                month_rows['status'].str.strip().str.lower().isin(['ìë£ŒëŒ€ê¸°', 'ìë£Œ ëŒ€ê¸°'])
            ])

            # monthly_summaryì— ì¶”ê°€
            monthly_summary.loc[monthly_summary['year_month'] == ym, 'completed_status_count'] = completed_count
            monthly_summary.loc[monthly_summary['year_month'] == ym, 'pending_data_count'] = pending_data_count
    else:
        monthly_summary = pd.DataFrame()

    # Get all individual work rows (for post_title, post_url display)
    # Filter rows that have valid post_title or post_url
    individual_posts = work_df[
        (work_df['post_title'].notna() & (work_df['post_title'] != '')) |
        (work_df['post_url'].notna() & (work_df['post_url'] != ''))
    ].copy()

    return {
        'monthly_summary': monthly_summary.to_dict('records') if not monthly_summary.empty else [],
        'work_summary': individual_posts.to_dict('records') if not individual_posts.empty else clinic_summary.to_dict('records'),
        'by_clinic': clinic_summary[['clinic', 'contract_count', 'published_count', 'remaining_count']].to_dict('records')
    }


def extract_month_from_filename(filename: str) -> Optional[str]:
    """Extract month from filename like '11ì›”' or '12ì›”' or date patterns."""
    # Pattern: 11ì›”, 12ì›”
    match = re.search(r'(\d{1,2})ì›”', filename)
    if match:
        month = int(match.group(1))
        # Assume current year context (2025)
        return f"2025-{month:02d}"

    # Pattern: 2025-12, 202512
    match = re.search(r'(\d{4})[-_]?(\d{2})', filename)
    if match:
        return f"{match.group(1)}-{match.group(2)}"

    return None


def process_inflow_xlsx(files: List[LoadedFile]) -> Dict[str, Any]:
    """
    Process inflow xlsx: 'ìœ ì…ë¶„ì„_ì›”ê°„_*.xlsx'
    ì¶”ê°€: URLì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ ê¸‰ìƒìŠ¹ ê²€ìƒ‰ì–´ TOP10 ìƒì„±
    ì›”ë³„ ë°ì´í„° ë¶„ë¦¬í•˜ì—¬ ì „ì›”/ë‹¹ì›” ë¹„êµ ê°€ëŠ¥
    """
    inflow_data = []
    search_keywords = []  # ê²€ìƒ‰ì–´ ìˆ˜ì§‘
    file_months = []

    for f in files:
        if not f.name.lower().endswith('.xlsx'):
            continue
        if 'ìœ ì…ë¶„ì„' not in f.name:
            continue

        # Extract month from filename
        file_month = extract_month_from_filename(f.name)
        if file_month:
            file_months.append(file_month)

        try:
            # Check df first, then raw_bytes
            if f.df is not None:
                df_raw = f.df.copy()
            elif f.raw_bytes:
                df_raw = pd.read_excel(BytesIO(f.raw_bytes), header=None)
            else:
                continue

            # Find header row containing 'ìœ ì…ê²½ë¡œ' and 'ë¹„ìœ¨'
            # In actual data: Row 7 has ['ìœ ì…ê²½ë¡œ', 'ë¹„ìœ¨', 'ìƒì„¸ìœ ì…ê²½ë¡œ', 'ë¹„ìœ¨']
            # ìœ ì…ê²½ë¡œ (ì²«ë²ˆì§¸ ì»¬ëŸ¼) ë°ì´í„°ë¥¼ ì‚¬ìš© (ìƒì„¸ìœ ì…ê²½ë¡œ ì•„ë‹˜)
            header_idx = -1
            for idx in range(min(15, len(df_raw))):
                row_values = [str(v).strip() for v in df_raw.iloc[idx].values if pd.notna(v)]
                if 'ìœ ì…ê²½ë¡œ' in row_values:
                    header_idx = idx
                    break

            if header_idx < 0:
                continue

            df = df_raw.iloc[header_idx + 1:].copy()
            df.columns = df_raw.iloc[header_idx].values
            df = df.reset_index(drop=True)

            # ì‹¤ì œ ì—‘ì…€ êµ¬ì¡° (ìœ ì…ë¶„ì„_ íŒŒì¼):
            # í—¤ë” í–‰ (8í–‰, ì¸ë±ìŠ¤ 7): ['ìœ ì…ê²½ë¡œ', 'ë¹„ìœ¨', 'ìƒì„¸ìœ ì…ê²½ë¡œ', 'ë¹„ìœ¨']
            # [0]ì—´ (Aì—´): ìœ ì…ê²½ë¡œ (ë„¤ì´ë²„ í†µí•©ê²€ìƒ‰, ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë“±) â˜… ì´ ë°ì´í„° ì‚¬ìš©
            # [1]ì—´ (Bì—´): ë¹„ìœ¨ (ìœ ì…ê²½ë¡œ ë¹„ìœ¨) â˜… ì´ ë°ì´í„° ì‚¬ìš©
            # [2]ì—´ (Cì—´): ìƒì„¸ìœ ì…ê²½ë¡œ (ê²€ìƒ‰ í‚¤ì›Œë“œ)
            # [3]ì—´ (Dì—´): ë¹„ìœ¨ (í‚¤ì›Œë“œ ë¹„ìœ¨)

            # ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
            col_names = [str(c).strip() if pd.notna(c) else '' for c in df.columns]

            # Aì—´(ìœ ì…ê²½ë¡œ)ê³¼ Bì—´(ë¹„ìœ¨)ì„ ì‚¬ìš©
            # ì²« ë²ˆì§¸ 'ìœ ì…ê²½ë¡œ' ì»¬ëŸ¼ ì°¾ê¸°
            inflow_col_idx = 0  # Aì—´: ìœ ì…ê²½ë¡œ
            inflow_ratio_col_idx = 1  # Bì—´: ë¹„ìœ¨

            for i, col_name in enumerate(col_names):
                if col_name == 'ìœ ì…ê²½ë¡œ':
                    inflow_col_idx = i
                    # ë°”ë¡œ ë‹¤ìŒ ì»¬ëŸ¼ì´ ë¹„ìœ¨
                    if i + 1 < len(col_names):
                        inflow_ratio_col_idx = i + 1
                    break

            # ë°ì´í„° ì¶”ì¶œ - Aì—´(ìœ ì…ê²½ë¡œ)ê³¼ Bì—´(ë¹„ìœ¨) ì‚¬ìš©
            for _, row in df.iterrows():
                if inflow_col_idx >= len(row) or inflow_ratio_col_idx >= len(row):
                    continue

                inflow_val = row.iloc[inflow_col_idx]
                ratio_val = row.iloc[inflow_ratio_col_idx]

                # ìœ ì…ê²½ë¡œ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸ (NaN, ë¹ˆ ë¬¸ìì—´ ì œì™¸)
                inflow_raw = str(inflow_val).strip() if pd.notna(inflow_val) else ''
                ratio = pd.to_numeric(str(ratio_val).replace('%', ''), errors='coerce') or 0

                # ìœ ì…ê²½ë¡œì— ê°’ì´ ìˆê³  ë¹„ìœ¨ë„ ìˆëŠ” í–‰ë§Œ ì¶”ê°€
                if inflow_raw and inflow_raw.lower() != 'nan' and ratio > 0:
                    inflow_data.append({
                        'source': inflow_raw,
                        'ratio': round(ratio, 2),
                        'file_month': file_month
                    })

        except Exception as e:
            print(f"Error processing inflow file {f.name}: {e}")
            continue

    if not inflow_data:
        return {}

    inflow_df = pd.DataFrame(inflow_data)
    sorted_months = sorted(set(file_months)) if file_months else []

    # ë¹„ìœ¨ ì •ê·œí™” í•¨ìˆ˜ (í•©ì´ 100%ê°€ ë˜ë„ë¡)
    def normalize_ratio(df_agg):
        """ë¹„ìœ¨ í•©ê³„ë¥¼ 100%ë¡œ ì •ê·œí™”"""
        total = df_agg['ratio'].sum()
        if total > 0:
            df_agg = df_agg.copy()
            df_agg['ratio'] = (df_agg['ratio'] / total * 100).round(1)
        return df_agg

    # ê¸°íƒ€ë¥¼ ë§¨ ì•„ë˜ë¡œ ë³´ë‚´ê³  TOP5 + ê¸°íƒ€ = 6ê°œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    def get_top5_with_etc(df_agg):
        """ê¸°íƒ€ë¥¼ ì œì™¸í•œ TOP5 + ê¸°íƒ€ = ì´ 6ê°œ ë°˜í™˜ (ë¹„ìœ¨ ì •ê·œí™” ì ìš©)"""
        # ë¨¼ì € ë¹„ìœ¨ ì •ê·œí™”
        df_agg = normalize_ratio(df_agg)

        etc_row = df_agg[df_agg['source'] == 'ê¸°íƒ€']
        non_etc = df_agg[df_agg['source'] != 'ê¸°íƒ€']
        top5_non_etc = non_etc.nlargest(5, 'ratio')
        # ê¸°íƒ€ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ì— ì¶”ê°€
        if not etc_row.empty:
            result = pd.concat([top5_non_etc, etc_row], ignore_index=True)
        else:
            result = top5_non_etc
        return result.to_dict('records')

    # ì›”ë³„ë¡œ TOP5 + ê¸°íƒ€ ë¶„ë¦¬
    monthly_traffic_top5 = {}
    for month in sorted_months:
        month_data = inflow_df[inflow_df['file_month'] == month]
        if not month_data.empty:
            month_agg = month_data.groupby('source')['ratio'].sum().reset_index()
            monthly_traffic_top5[month] = get_top5_with_etc(month_agg)

    # ì „ì²´ ì§‘ê³„ (í•˜ìœ„ í˜¸í™˜ì„±)
    inflow_agg = inflow_df.groupby('source')['ratio'].sum().reset_index()
    top5 = get_top5_with_etc(inflow_agg)

    # ê²€ìƒ‰ì–´ TOP10 ì§‘ê³„ (ì›”ë³„)
    search_keywords_top10 = []
    monthly_search_keywords = {}
    if search_keywords:
        kw_df = pd.DataFrame(search_keywords)
        # ì›”ë³„ ê²€ìƒ‰ì–´
        for month in sorted_months:
            month_kw = kw_df[kw_df['file_month'] == month]
            if not month_kw.empty:
                month_kw_agg = month_kw.groupby('keyword')['ratio'].sum().reset_index()
                month_kw_agg = month_kw_agg.sort_values('ratio', ascending=False)
                monthly_search_keywords[month] = month_kw_agg.head(10).to_dict('records')

        # ì „ì²´ ê²€ìƒ‰ì–´
        kw_agg = kw_df.groupby('keyword')['ratio'].sum().reset_index()
        kw_agg = kw_agg.sort_values('ratio', ascending=False)
        search_keywords_top10 = kw_agg.head(10).to_dict('records')

    return {
        'traffic_top5': top5,
        'search_keywords_top10': search_keywords_top10,  # ê¸‰ìƒìŠ¹ ê²€ìƒ‰ì–´ TOP10
        'file_months': sorted_months,
        'monthly_traffic_top5': monthly_traffic_top5,  # ì›”ë³„ íŠ¸ë˜í”½ TOP5
        'monthly_search_keywords': monthly_search_keywords  # ì›”ë³„ ê²€ìƒ‰ì–´ TOP10
    }


def process_views_rank_xlsx(files: List[LoadedFile], analysis_month: str = None) -> Dict[str, Any]:
    """
    Process views rank xlsx: 'ì¡°íšŒìˆ˜_ìˆœìœ„_ì›”ê°„_*.xlsx'
    ì¶”ê°€: ìŠ¤í…Œë””ì…€ëŸ¬(íš¨ì ì½˜í…ì¸ ) ë°œêµ´ - ì‘ì„±ì¼ì´ ë¶„ì„ ì›” ì´ì „ì¸ ì¸ê¸° ê²Œì‹œë¬¼ í‘œì‹œ
    ì›”ë³„ ë°ì´í„° ë¶„ë¦¬í•˜ì—¬ ì „ì›”/ë‹¹ì›” ë¹„êµ ê°€ëŠ¥
    """
    views_data = []
    file_months = []

    for f in files:
        if not f.name.lower().endswith('.xlsx'):
            continue
        if 'ì¡°íšŒìˆ˜_ìˆœìœ„' not in f.name:
            continue

        # Extract month from filename
        file_month = extract_month_from_filename(f.name)
        if file_month:
            file_months.append(file_month)

        try:
            # Check df first, then raw_bytes
            if f.df is not None:
                df_raw = f.df.copy()
            elif f.raw_bytes:
                df_raw = pd.read_excel(BytesIO(f.raw_bytes), header=None)
            else:
                continue

            # Find header row containing 'ìˆœìœ„', 'ì¡°íšŒìˆ˜'
            # Row 7 has: ['ìˆœìœ„', 'ì œëª©', 'ì¡°íšŒìˆ˜', 'ì‘ì„±ì¼']
            header_idx = -1
            for idx in range(min(15, len(df_raw))):
                row_values = [str(v).strip() for v in df_raw.iloc[idx].values if pd.notna(v)]
                if 'ìˆœìœ„' in row_values and 'ì¡°íšŒìˆ˜' in row_values:
                    header_idx = idx
                    break

            if header_idx < 0:
                continue

            df = df_raw.iloc[header_idx + 1:].copy()
            df.columns = df_raw.iloc[header_idx].values
            df = df.reset_index(drop=True)

            # Find columns
            rank_col = None
            views_col = None
            title_col = None
            write_date_col = None

            for col in df.columns:
                if pd.isna(col):
                    continue
                col_str = str(col).strip()
                if col_str == 'ìˆœìœ„':
                    rank_col = col
                elif col_str == 'ì¡°íšŒìˆ˜':
                    views_col = col
                elif col_str == 'ì œëª©':
                    title_col = col
                elif col_str == 'ì‘ì„±ì¼':
                    write_date_col = col

            if views_col and title_col:
                for _, row in df.iterrows():
                    views = pd.to_numeric(row.get(views_col, 0), errors='coerce') or 0
                    title = str(row.get(title_col, '')).strip()
                    rank = pd.to_numeric(row.get(rank_col, 0), errors='coerce') or 0 if rank_col else 0
                    write_date = str(row.get(write_date_col, '')).strip() if write_date_col else ''

                    if views > 0 and title and title.lower() != 'nan':
                        # ìŠ¤í…Œë””ì…€ëŸ¬ ì—¬ë¶€ íŒë‹¨
                        is_steady = is_steady_seller(write_date, analysis_month) if analysis_month else False

                        views_data.append({
                            'rank': int(rank),
                            'title': title,
                            'views': int(views),
                            'write_date': write_date if write_date.lower() != 'nan' else '',
                            'is_steady_seller': is_steady,
                            'file_month': file_month
                        })

        except Exception as e:
            print(f"Error processing views rank file {f.name}: {e}")
            continue

    if not views_data:
        return {}

    views_df = pd.DataFrame(views_data)
    sorted_months = sorted(set(file_months)) if file_months else []

    # ì›”ë³„ë¡œ TOP5 ë¶„ë¦¬
    monthly_views_top5 = {}
    for month in sorted_months:
        month_data = views_df[views_df['file_month'] == month]
        if not month_data.empty:
            monthly_views_top5[month] = month_data.nlargest(5, 'views').to_dict('records')

    # ì „ì²´ ì§‘ê³„ (í•˜ìœ„ í˜¸í™˜ì„±)
    top10 = views_df.nlargest(10, 'views').to_dict('records')
    top5 = views_df.nlargest(5, 'views').to_dict('records')

    # ìŠ¤í…Œë””ì…€ëŸ¬ë§Œ í•„í„°ë§
    steady_sellers = [v for v in top10 if v.get('is_steady_seller', False)]

    return {
        'views_top5': top5,
        'views_top10': top10,
        'steady_sellers': steady_sellers,  # íš¨ì ì½˜í…ì¸ (ìŠ¤í…Œë””ì…€ëŸ¬)
        'file_months': sorted_months,
        'monthly_views_top5': monthly_views_top5  # ì›”ë³„ ì¡°íšŒìˆ˜ TOP5
    }


def process_views_monthly_xlsx(files: List[LoadedFile]) -> Dict[str, Any]:
    """Process views monthly xlsx: 'ì¡°íšŒìˆ˜_ì›”ê°„_*.xlsx'"""
    monthly_views = []

    for f in files:
        if not f.name.lower().endswith('.xlsx'):
            continue
        # Match ì¡°íšŒìˆ˜_ì›”ê°„_ but not ì¡°íšŒìˆ˜_ìˆœìœ„_ì›”ê°„_
        if 'ì¡°íšŒìˆ˜_ì›”ê°„_' not in f.name or 'ìˆœìœ„' in f.name:
            continue

        try:
            # Check df first, then raw_bytes
            if f.df is not None:
                df_raw = f.df.copy()
            elif f.raw_bytes:
                df_raw = pd.read_excel(BytesIO(f.raw_bytes), header=None)
            else:
                continue

            # Find header row containing 'ê¸°ê°„' and 'ì „ì²´'
            # Row 6 has: ['ê¸°ê°„', 'ê¸°ê°„', 'ì „ì²´', 'í”¼ì´ì›ƒ', 'ì„œë¡œì´ì›ƒ', 'ê¸°íƒ€']
            header_idx = -1
            for idx in range(min(15, len(df_raw))):
                row_values = [str(v).strip() for v in df_raw.iloc[idx].values if pd.notna(v)]
                if 'ê¸°ê°„' in row_values and 'ì „ì²´' in row_values:
                    header_idx = idx
                    break

            if header_idx < 0:
                continue

            df = df_raw.iloc[header_idx + 1:].copy()
            df.columns = df_raw.iloc[header_idx].values
            df = df.reset_index(drop=True)

            # Find period and total columns
            # Data row: ['2025-12-01~2025-12-31', '739', '5', '64', '670']
            # The first non-nan value containing date range is the period
            # The 'ì „ì²´' column has the total views

            period_col = None
            total_col = None

            col_list = list(df.columns)
            for i, col in enumerate(col_list):
                if pd.isna(col):
                    continue
                col_str = str(col).strip()
                if col_str == 'ê¸°ê°„' and period_col is None:
                    period_col = col
                elif col_str == 'ì „ì²´':
                    total_col = col

            if total_col:
                for _, row in df.iterrows():
                    # Get the first column value as period (it contains the date range)
                    period = str(row.iloc[0]).strip() if len(row) > 0 else ''
                    year_month = parse_date_range_to_year_month(period)
                    total_views = pd.to_numeric(row.get(total_col, 0), errors='coerce') or 0

                    if year_month and total_views > 0:
                        monthly_views.append({
                            'year_month': year_month,
                            'total_views': int(total_views)
                        })

        except Exception as e:
            print(f"Error processing views monthly file {f.name}: {e}")
            continue

    if not monthly_views:
        return {}

    views_df = pd.DataFrame(monthly_views)
    views_df = views_df.groupby('year_month')['total_views'].sum().reset_index()
    views_df = views_df.sort_values('year_month')
    views_df['mom_growth'] = views_df['total_views'].pct_change() * 100

    return {
        'monthly_views': views_df.to_dict('records'),
        'total_by_month': dict(zip(views_df['year_month'], views_df['total_views']))
    }


def process_blog(files: List[LoadedFile]) -> Dict[str, Any]:
    """
    Main processor for blog/content team.

    Args:
        files: List of LoadedFile objects

    Returns:
        dict with department, month, prev_month, current_month_data, prev_month_data,
        growth_rate, kpi, tables, charts, clean_data, diagnosis, insights

    ë¶„ì„ ë¡œì§:
    1. ì´ì›” ê±´ìˆ˜: ê³„ì•½ ê±´ìˆ˜ - ë°œí–‰ ì™„ë£Œ ê±´ìˆ˜
    2. í¬ìŠ¤íŒ… ë¦¬ìŠ¤íŠ¸: ì œëª©ê³¼ URL (í´ë¦­ ê°€ëŠ¥í•œ ë§í¬)
    3. ì „ì›” ëŒ€ë¹„ ì¡°íšŒìˆ˜ ì¦ê°ë¥  ê³„ì‚°
    """
    work_result = process_work_csv(files)
    inflow_result = process_inflow_xlsx(files)
    views_monthly_result = process_views_monthly_xlsx(files)

    # Determine months first (views_rank needs current_month for steady_seller detection)
    all_months = set()
    if work_result.get('monthly_summary'):
        all_months.update([s['year_month'] for s in work_result['monthly_summary'] if s.get('year_month')])
    if views_monthly_result.get('monthly_views'):
        all_months.update([s['year_month'] for s in views_monthly_result['monthly_views']])

    sorted_months = sorted(all_months) if all_months else []
    current_month = sorted_months[-1] if sorted_months else None
    prev_month = sorted_months[-2] if len(sorted_months) >= 2 else None

    # Process views rank with analysis_month for steady seller detection
    views_rank_result = process_views_rank_xlsx(files, analysis_month=current_month)

    # Current month data
    current_month_data = {}
    prev_month_data = {}

    if work_result.get('monthly_summary'):
        for summary in work_result['monthly_summary']:
            if summary.get('year_month') == current_month:
                current_month_data['work'] = summary
            elif summary.get('year_month') == prev_month:
                prev_month_data['work'] = summary

    if views_monthly_result.get('total_by_month'):
        current_month_data['total_views'] = views_monthly_result['total_by_month'].get(current_month, 0)
        prev_month_data['total_views'] = views_monthly_result['total_by_month'].get(prev_month, 0)

    # Growth rate
    growth_rate = {}
    curr_views = current_month_data.get('total_views', 0)
    prev_views = prev_month_data.get('total_views', 0)
    if prev_views > 0:
        growth_rate['views'] = ((curr_views - prev_views) / prev_views) * 100
    else:
        growth_rate['views'] = 0

    # KPI
    work_summary = current_month_data.get('work', {})
    curr_publish_count = work_summary.get('published_count', 0)
    contract_count = work_summary.get('contract_count', 0)

    # ì´ì›” ê±´ìˆ˜ ì·¨í•©: "ì§€ë‚œë‹¬ ì´ì›” ê±´ìˆ˜" ì»¬ëŸ¼ ë°ì´í„° ì‚¬ìš© (User Request Reversion)
    carryover_count = work_summary.get('base_carryover', 0)

    # ì „ì›” ë°ì´í„°
    prev_work_summary = prev_month_data.get('work', {})
    prev_publish_count = prev_work_summary.get('published_count', 0)
    prev_contract_count = prev_work_summary.get('contract_count', 0)
    prev_carryover_count = prev_work_summary.get('base_carryover', 0)

    kpi = {
        'publish_completion_rate': round(work_summary.get('completion_rate', 0), 2),
        'remaining_cnt': work_summary.get('remaining_count', 0),
        'total_views': current_month_data.get('total_views', 0),
        'views_mom_growth': round(growth_rate.get('views', 0), 2),
        'published_count': curr_publish_count,  # ë°œí–‰ëŸ‰
        'contract_count': contract_count,  # ê³„ì•½ ê±´ìˆ˜
        'carryover_count': carryover_count,  # ì´ì›” ê±´ìˆ˜ (ê³„ì•½-ë°œí–‰)
        'pending_data_count': work_summary.get('pending_data_count', 0),  # ìë£Œ ë¯¸ìˆ˜ì‹  ê±´ìˆ˜
        # ì „ì›” ë°ì´í„°
        'prev_published_count': prev_publish_count,
        'prev_contract_count': prev_contract_count,
        'prev_carryover_count': prev_carryover_count,
        'prev_total_views': prev_month_data.get('total_views', 0)
    }

    # ì„±ê³¼ ì›ì¸ ìë™ ì§„ë‹¨
    diagnosis = generate_performance_diagnosis(
        curr_views=curr_views,
        prev_views=prev_views,
        curr_publish_count=curr_publish_count,
        prev_publish_count=prev_month_data.get('work', {}).get('published_count', 0)
    )

    # Separate work_summary by month for side-by-side comparison
    all_work_summary = work_result.get('work_summary', [])
    work_df = pd.DataFrame(all_work_summary) if all_work_summary else pd.DataFrame()

    curr_work_summary = []
    prev_work_summary = []

    if not work_df.empty and 'year_month' in work_df.columns:
        curr_work_df = work_df[work_df['year_month'] == current_month]
        prev_work_df = work_df[work_df['year_month'] == prev_month]
        curr_work_summary = curr_work_df.to_dict('records') if not curr_work_df.empty else []
        prev_work_summary = prev_work_df.to_dict('records') if not prev_work_df.empty else []

    # ì›”ë³„ TOP5 ë°ì´í„° ì¶”ì¶œ (posting_listì—ì„œ ë°œí–‰ì¼ ë§¤í•‘ì— ì‚¬ìš©)
    monthly_views_top5 = views_rank_result.get('monthly_views_top5', {})

    # ë‹¹ì›”/ì „ì›” views_top5 ë¨¼ì € ê°€ì ¸ì˜¤ê¸° (ë°œí–‰ì¼ ë§¤í•‘ìš©)
    curr_views_top5 = monthly_views_top5.get(current_month, views_rank_result.get('views_top5', []))
    prev_views_top5 = monthly_views_top5.get(prev_month, [])

    # views_top5ì—ì„œ ì œëª© -> ë°œí–‰ì¼ ë§¤í•‘ ìƒì„±
    curr_views_date_map = {post.get('title', ''): post.get('write_date', '') for post in curr_views_top5}
    prev_views_date_map = {post.get('title', ''): post.get('write_date', '') for post in prev_views_top5}

    # í¬ìŠ¤íŒ… ë¦¬ìŠ¤íŠ¸ - ì œëª©, URL, ë°œí–‰ì¼ ì •ë¦¬ (í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ìš©)
    posting_list = []
    for post in curr_work_summary:
        title = post.get('post_title', '')
        url = post.get('post_url', '')
        status = post.get('status', '')
        # ì§ì ‘ upload_date ì‚¬ìš© (ì—†ìœ¼ë©´ views_top5ì—ì„œ ì°¾ê¸°)
        write_date = post.get('upload_date', '')
        if not write_date:
            write_date = curr_views_date_map.get(title, '')
        if title and title.lower() != 'nan':
            posting_list.append({
                'title': title,
                'url': url if url and url.lower() != 'nan' else '',
                'status': status,
                'write_date': write_date  # ë°œí–‰ì¼ ì¶”ê°€
            })

    prev_posting_list = []
    for post in prev_work_summary:
        title = post.get('post_title', '')
        url = post.get('post_url', '')
        status = post.get('status', '')
        # ì§ì ‘ upload_date ì‚¬ìš© (ì—†ìœ¼ë©´ views_top5ì—ì„œ ì°¾ê¸°)
        write_date = post.get('upload_date', '')
        if not write_date:
            write_date = prev_views_date_map.get(title, '')
        if title and title.lower() != 'nan':
            prev_posting_list.append({
                'title': title,
                'url': url if url and url.lower() != 'nan' else '',
                'status': status,
                'write_date': write_date  # ë°œí–‰ì¼ ì¶”ê°€
            })

    monthly_traffic_top5 = inflow_result.get('monthly_traffic_top5', {})

    # ë‹¹ì›”/ì „ì›” traffic_top5
    curr_traffic_top5 = monthly_traffic_top5.get(current_month, inflow_result.get('traffic_top5', []))
    prev_traffic_top5 = monthly_traffic_top5.get(prev_month, [])

    # Tables with new features
    tables = {
        'traffic_top5': curr_traffic_top5,
        'prev_traffic_top5': prev_traffic_top5,  # ì „ì›” íŠ¸ë˜í”½ TOP5
        'views_top5': curr_views_top5,
        'prev_views_top5': prev_views_top5,  # ì „ì›” ì¡°íšŒìˆ˜ TOP5
        'views_top10': views_rank_result.get('views_top10', []),
        'work_summary': all_work_summary,
        'curr_work_summary': curr_work_summary,
        'prev_work_summary': prev_work_summary,
        # í¬ìŠ¤íŒ… ë¦¬ìŠ¤íŠ¸ (ì œëª© + URL)
        'posting_list': posting_list,
        'prev_posting_list': prev_posting_list,
        # ìƒˆë¡œìš´ ë¶„ì„ ë°ì´í„°
        'search_keywords_top10': inflow_result.get('search_keywords_top10', []),  # ê¸‰ìƒìŠ¹ ê²€ìƒ‰ì–´
        'steady_sellers': views_rank_result.get('steady_sellers', []),  # íš¨ì ì½˜í…ì¸ 
        # ì›”ë³„ ë°ì´í„° (ìƒì„¸)
        'monthly_views_top5': monthly_views_top5,
        'monthly_traffic_top5': monthly_traffic_top5
    }

    # Charts
    charts = {
        'views_trend': views_monthly_result.get('monthly_views', [])
    }

    # Clean data
    clean_data = {
        'work': work_result,
        'inflow': inflow_result,
        'views_rank': views_rank_result,
        'views_monthly': views_monthly_result
    }

    # ì¸ì‚¬ì´íŠ¸ ìƒì„±
    insights = {
        'diagnosis': diagnosis,
        'has_steady_sellers': len(views_rank_result.get('steady_sellers', [])) > 0,
        'steady_seller_count': len(views_rank_result.get('steady_sellers', [])),
        'has_search_keywords': len(inflow_result.get('search_keywords_top10', [])) > 0,
        'search_keyword_count': len(inflow_result.get('search_keywords_top10', []))
    }

    return {
        'department': 'ì½˜í…ì¸ íŒ€',
        'month': current_month,
        'prev_month': prev_month,
        'current_month_data': current_month_data,
        'prev_month_data': prev_month_data,
        'growth_rate': growth_rate,
        'kpi': kpi,
        'tables': tables,
        'charts': charts,
        'clean_data': clean_data,
        'diagnosis': diagnosis,  # ì„±ê³¼ ì›ì¸ ì§„ë‹¨
        'insights': insights  # ì¸ì‚¬ì´íŠ¸
    }
